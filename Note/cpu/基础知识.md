![[进程状态.png]]
### TLB miss

CPU访问某个虚拟内存地址的过程如下

- 1.CPU产生一个虚拟地址
    
- 2.MMU从TLB中获取页表，翻译成物理地址
    
- 3.MMU把物理地址发送给L1/L2/L3/内存
    
- 4.L1/L2/L3/内存将地址对应数据返回给CPU


> [!NOTE] 为什么需要TLB
> 64位系统的虚拟内存实现：四级页表
> 一次内存io请求就需要四次页表的访问io+真正的内存访问io

> [!NOTE] 使用大内存页
> 因为TLB并不是很大，只有4k，而且现在逻辑核又造成会有两个进程来共享。所以可能会有cache miss的情况出现。而且一旦TLB miss造成的后果可比物理地址cache miss后果要严重一些，最多可能需要进行5次内存IO才行。建议你先用上面的perf工具查看一下你的程序的TLB的miss情况，如果确实不命中率很高，那么Linux允许你使用大内存页

## 内核cpu开销
#### 进程/线程切换的开销(上下文切换)
1. 切换页表全局目录
2. 切换内核态堆栈
3. 切换硬件上下文（寄存器相关数据）
4. 刷新TLB
5. 系统调度器代码执行

> [!NOTE] 间接开销
> 跨cpu调度导致新进程穿透到内存的io会变多


> [!NOTE] 查看上下文开销的命令
> vmstat -1
> sar -w -1
![[vwstat.png]]
![[sar.png]]
`proc/s` 代表**每秒创建的进程数**
`cswch/s` 代表**每秒上下文切换次数**

#### 系统调用的开销
1. 切换到同进程的内核态上下文
2. 进入内核态继续工作

#### 软中断的开销
1. 切换到了另外一个内核进程ksoftirqd上
2. 进入内核态继续工作

> [!NOTE] 软中断开销与系统调用开销的区别
>把内核看做是不断对请求进行响应的服务器，这些请求可能来自在CPU上执行的进程，也可能来自发出中断的外部设备。老板的请求相当于中断，**而顾客的请求相当于用户态进程发出的系统调用**
>- 软中断必须经过一致性和安全性检查，所以中断的开销要比系统调用的开销多
>- 系统调用是用了Intel提供的“快速系统调用”的sysenter指令

> [!NOTE] 进程/线程上下文切换与软中断上下文切换的区别
> 进程上下文切换是从用户进程A切换到了用户进程B。而软中断切换是从用户进程A切换到了内核线程ksoftirqd上。而ksoftirqd作为一个内核控制路径，其处理程序比一个用户进程要轻量，所以上下文切换开销相对比进程切换要稍一些。

#### CPU使用率
1. 如何计算
	1. 毫秒级别采样，将每个cpu的核是否使用通过微妙采样并在一个时间段进行平均
2. 结果存放
	1. 在/proc/stat

## 容器下的CPU利用率
#### lxcfs
	lxcfs实现的基本原理是通过文件挂载的方式，把cgroup中容器相关的信息读取出来，存储到lxcfs相关的目录下，并将相关目录映射到容器内的/proc目录下，从而使得容器内执行top,free等命令时拿到的/proc下的数据是真实的cgroup分配给容器的CPU和内存数据。
![[lxcfs.png]]

#### cadvisor
	找到容器所属的 cgroup 目录，在这里也有当前 cgroup 所消耗的 cpu 资源的统计信息。根据这个信息可以计算出容器的 cpu 利用率。
	cadvisor 就是采用上述方案来上报容器 cpu 利用率的打点信息的。每隔一段定长的时间都进行采样，将数据上传给 Prometheus

> [!NOTE] 访问cgroup目录
>  1.cadvisor 具体访问 cgroup 目录是通过调用 [libcontainer](https://github.com/opencontainers/runc/tree/main/libcontainer) 获取的
>  2.libcontainer 中是通过 cgroup 挂载的文件系统的 magic number 来判断的